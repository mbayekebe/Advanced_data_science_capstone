{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ETL Process\n",
    "# For this project the ETL process is quite simple\n",
    "# 1.The dataset was downloaded from the website of the Namibia Statistics Agency \n",
    "# 2. The downloaded file was in SPSS format\n",
    "# 3. It SPSS format was transformed into a CSV \n",
    "# 4. THe CSV then uploaded to CloudObject Storage in my IBM Watson Studio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': 'YqS2Jsc_fizSkQk91m7T3pBghp0vVXgTIvl-dtcKTR68',\n",
    "    'service_id': 'iam-ServiceId-a9d9933e-9803-4a3b-b387-dc29a5129872',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_d80919c85ea34f148938b12fae700e99_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('NHIESHH2016.csv', 'scalableds-donotdelete-pr-tmcow79opbwcdy'))\n",
    "#df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|apci_groups|\n",
      "+-----------+\n",
      "|          3|\n",
      "|          3|\n",
      "|          5|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          2|\n",
      "|          5|\n",
      "|          1|\n",
      "|          2|\n",
      "|          2|\n",
      "|          1|\n",
      "|          3|\n",
      "|          6|\n",
      "|          4|\n",
      "|          3|\n",
      "|          5|\n",
      "|          4|\n",
      "|          6|\n",
      "|          3|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT apci_groups from df\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhiesmdl = spark.sql(\"SELECT sex_of_head, main_language, main_sourceincome,apci_groups from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+-----------+\n",
      "|sex_of_head|main_language|main_sourceincome|apci_groups|\n",
      "+-----------+-------------+-----------------+-----------+\n",
      "|          2|            5|                1|          3|\n",
      "|          2|           10|                5|          3|\n",
      "|          1|            5|                1|          5|\n",
      "|          2|            5|                5|          3|\n",
      "|          1|           12|                1|          3|\n",
      "+-----------+-------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nhiesmdl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv to be retrieved later\n",
    "nhiesmdl.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"data/home/hiesmdl1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "#sc = pyspark.SparkContext()\n",
    "sql = SQLContext(sc)\n",
    "#read back saved csv\n",
    "dfn = (sql.read\n",
    "         .format(\"com.databricks.spark.csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .load(\"data/home/hiesmdl1.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+-----------+\n",
      "|sex_of_head|main_language|main_sourceincome|apci_groups|\n",
      "+-----------+-------------+-----------------+-----------+\n",
      "|          2|            5|                1|          3|\n",
      "|          2|           10|                5|          3|\n",
      "|          1|            5|                1|          5|\n",
      "|          2|            5|                5|          3|\n",
      "|          1|           12|                1|          3|\n",
      "+-----------+-------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfn.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|apci_groups|\n",
      "+-----------+\n",
      "|          7|\n",
      "|          3|\n",
      "|          5|\n",
      "|          6|\n",
      "|          1|\n",
      "|          4|\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfn.createOrReplaceTempView(\"dfn\")\n",
    "result = spark.sql(\"\"\"SELECT apci_groups FROM dfn GROUP BY apci_groups\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sex_of_head', 'string'),\n",
       " ('main_language', 'string'),\n",
       " ('main_sourceincome', 'string'),\n",
       " ('apci_groups', 'string')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import os\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dfn.columns[:-1]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dfn) for column in list(dfn.columns) ]    #list(set(dfn.columns)-set(['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----------------+\n",
      "|sex_of_head|main_language|main_sourceincome|apci_groups|sex_of_head_index|main_language_index|main_sourceincome_index|apci_groups_index|\n",
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----------------+\n",
      "|          2|            5|                1|          3|              0.0|                1.0|                    0.0|              0.0|\n",
      "|          2|           10|                5|          3|              0.0|                8.0|                    3.0|              0.0|\n",
      "|          1|            5|                1|          5|              1.0|                1.0|                    0.0|              4.0|\n",
      "|          2|            5|                5|          3|              0.0|                1.0|                    3.0|              0.0|\n",
      "|          1|           12|                1|          3|              1.0|                9.0|                    0.0|              0.0|\n",
      "|          1|            8|                1|          3|              1.0|                4.0|                    0.0|              0.0|\n",
      "|          2|            5|                2|          2|              0.0|                1.0|                    1.0|              1.0|\n",
      "|          2|            5|                1|          5|              0.0|                1.0|                    0.0|              4.0|\n",
      "|          2|            5|                1|          1|              0.0|                1.0|                    0.0|              3.0|\n",
      "|          2|            5|                1|          2|              0.0|                1.0|                    0.0|              1.0|\n",
      "|          1|            8|                2|          2|              1.0|                4.0|                    1.0|              1.0|\n",
      "|          2|            5|                2|          1|              0.0|                1.0|                    1.0|              3.0|\n",
      "|          2|            8|                1|          3|              0.0|                4.0|                    0.0|              0.0|\n",
      "|          1|            8|                1|          6|              1.0|                4.0|                    0.0|              5.0|\n",
      "|          2|            8|                1|          4|              0.0|                4.0|                    0.0|              2.0|\n",
      "|          2|            6|                1|          3|              0.0|                0.0|                    0.0|              0.0|\n",
      "|          2|            3|                1|          5|              0.0|                3.0|                    0.0|              4.0|\n",
      "|          2|            8|                1|          4|              0.0|                4.0|                    0.0|              2.0|\n",
      "|          1|            8|                1|          6|              1.0|                4.0|                    0.0|              5.0|\n",
      "|          2|            4|                1|          3|              0.0|                2.0|                    0.0|              0.0|\n",
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(dfn).transform(dfn)\n",
    "\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df_r.withColumnRenamed(\"apci_groups_index\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# One Hot Encoder on indexed features\n",
    "df_r = OneHotEncoder(inputCol=\"sex_of_head_index\", outputCol=\"sexVec\").transform(df_r)\n",
    "df_r = OneHotEncoder(inputCol=\"main_language_index\", outputCol=\"languageVec\").transform(df_r)\n",
    "df_r = OneHotEncoder(inputCol=\"main_sourceincome_index\", outputCol=\"incsourceVec\").transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----+-------------+--------------+-------------+--------------------+\n",
      "|sex_of_head|main_language|main_sourceincome|apci_groups|sex_of_head_index|main_language_index|main_sourceincome_index|label|       sexVec|   languageVec| incsourceVec|            features|\n",
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----+-------------+--------------+-------------+--------------------+\n",
      "|          2|            5|                1|          3|              0.0|                1.0|                    0.0|  0.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[0],[1.0])|(20,[0,2,13],[1.0...|\n",
      "|          2|           10|                5|          3|              0.0|                8.0|                    3.0|  0.0|(1,[0],[1.0])|(12,[8],[1.0])|(7,[3],[1.0])|(20,[0,9,16],[1.0...|\n",
      "|          1|            5|                1|          5|              1.0|                1.0|                    0.0|  4.0|    (1,[],[])|(12,[1],[1.0])|(7,[0],[1.0])|(20,[2,13],[1.0,1...|\n",
      "|          2|            5|                5|          3|              0.0|                1.0|                    3.0|  0.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(20,[0,2,16],[1.0...|\n",
      "|          1|           12|                1|          3|              1.0|                9.0|                    0.0|  0.0|    (1,[],[])|(12,[9],[1.0])|(7,[0],[1.0])|(20,[10,13],[1.0,...|\n",
      "|          1|            8|                1|          3|              1.0|                4.0|                    0.0|  0.0|    (1,[],[])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[5,13],[1.0,1...|\n",
      "|          2|            5|                2|          2|              0.0|                1.0|                    1.0|  1.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(20,[0,2,14],[1.0...|\n",
      "|          2|            5|                1|          5|              0.0|                1.0|                    0.0|  4.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[0],[1.0])|(20,[0,2,13],[1.0...|\n",
      "|          2|            5|                1|          1|              0.0|                1.0|                    0.0|  3.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[0],[1.0])|(20,[0,2,13],[1.0...|\n",
      "|          2|            5|                1|          2|              0.0|                1.0|                    0.0|  1.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[0],[1.0])|(20,[0,2,13],[1.0...|\n",
      "|          1|            8|                2|          2|              1.0|                4.0|                    1.0|  1.0|    (1,[],[])|(12,[4],[1.0])|(7,[1],[1.0])|(20,[5,14],[1.0,1...|\n",
      "|          2|            5|                2|          1|              0.0|                1.0|                    1.0|  3.0|(1,[0],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(20,[0,2,14],[1.0...|\n",
      "|          2|            8|                1|          3|              0.0|                4.0|                    0.0|  0.0|(1,[0],[1.0])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[0,5,13],[1.0...|\n",
      "|          1|            8|                1|          6|              1.0|                4.0|                    0.0|  5.0|    (1,[],[])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[5,13],[1.0,1...|\n",
      "|          2|            8|                1|          4|              0.0|                4.0|                    0.0|  2.0|(1,[0],[1.0])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[0,5,13],[1.0...|\n",
      "|          2|            6|                1|          3|              0.0|                0.0|                    0.0|  0.0|(1,[0],[1.0])|(12,[0],[1.0])|(7,[0],[1.0])|(20,[0,1,13],[1.0...|\n",
      "|          2|            3|                1|          5|              0.0|                3.0|                    0.0|  4.0|(1,[0],[1.0])|(12,[3],[1.0])|(7,[0],[1.0])|(20,[0,4,13],[1.0...|\n",
      "|          2|            8|                1|          4|              0.0|                4.0|                    0.0|  2.0|(1,[0],[1.0])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[0,5,13],[1.0...|\n",
      "|          1|            8|                1|          6|              1.0|                4.0|                    0.0|  5.0|    (1,[],[])|(12,[4],[1.0])|(7,[0],[1.0])|(20,[5,13],[1.0,1...|\n",
      "|          2|            4|                1|          3|              0.0|                2.0|                    0.0|  0.0|(1,[0],[1.0])|(12,[2],[1.0])|(7,[0],[1.0])|(20,[0,3,13],[1.0...|\n",
      "+-----------+-------------+-----------------+-----------+-----------------+-------------------+-----------------------+-----+-------------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature assembler as a vector\n",
    "assembler = VectorAssembler(inputCols = ['sexVec','languageVec','incsourceVec'],outputCol = 'features')\n",
    "df_r = assembler.transform(df_r)\n",
    "df_r.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_r.randomSplit([0.7, 0.3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
